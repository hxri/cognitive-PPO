C:\Users\hxri\miniconda3\envs\cogppo\Lib\site-packages\gymnasium\envs\registration.py:523: DeprecationWarning: [33mWARN: The environment MiniGrid-Dynamic-Obstacles-Random-10x10-v0 is out of date. You should consider upgrading to version `v2`.
  logger.deprecation(
global_step=8, episodic_return=[-1.]
global_step=28, episodic_return=[-1.]
global_step=44, episodic_return=[-1.]
global_step=52, episodic_return=[-1.]
global_step=56, episodic_return=[-1.]
global_step=60, episodic_return=[-1.]
global_step=68, episodic_return=[-1.]
global_step=72, episodic_return=[-1.]
global_step=96, episodic_return=[-1.]
Moviepy - Building video D:\Masters\Projects\thesis\cognitive-PPO\videos\MiniGrid-Dynamic-Obstacles-Random-10x10-v0__playground_gridworld__1__1683220536\rl-video-episode-0.mp4.
Moviepy - Writing video D:\Masters\Projects\thesis\cognitive-PPO\videos\MiniGrid-Dynamic-Obstacles-Random-10x10-v0__playground_gridworld__1__1683220536\rl-video-episode-0.mp4
Moviepy - Done !
Moviepy - video ready D:\Masters\Projects\thesis\cognitive-PPO\videos\MiniGrid-Dynamic-Obstacles-Random-10x10-v0__playground_gridworld__1__1683220536\rl-video-episode-0.mp4
global_step=104, episodic_return=[-1.]
global_step=112, episodic_return=[-1.]
global_step=132, episodic_return=[-1.]
global_step=144, episodic_return=[-1.]
global_step=196, episodic_return=[-1.]
global_step=224, episodic_return=[-1.]
global_step=260, episodic_return=[-1.]
global_step=292, episodic_return=[-1.]
global_step=296, episodic_return=[-1.]
global_step=300, episodic_return=[-1.]
global_step=312, episodic_return=[-1.]
global_step=380, episodic_return=[-1.]
global_step=392, episodic_return=[-1.]
global_step=408, episodic_return=[-1.]
global_step=436, episodic_return=[0.99937]
global_step=476, episodic_return=[-1.]
global_step=480, episodic_return=[-1.]
global_step=488, episodic_return=[-1.]
global_step=508, episodic_return=[-1.]
SPS:  91
global_step=548, episodic_return=[-1.]
global_step=568, episodic_return=[-1.]
global_step=644, episodic_return=[-1.]
global_step=672, episodic_return=[-1.]
global_step=712, episodic_return=[-1.]
global_step=736, episodic_return=[-1.]
global_step=748, episodic_return=[-1.]
global_step=776, episodic_return=[-1.]
global_step=788, episodic_return=[-1.]
global_step=856, episodic_return=[-1.]
global_step=900, episodic_return=[-1.]
global_step=936, episodic_return=[-1.]
global_step=948, episodic_return=[-1.]
global_step=992, episodic_return=[-1.]
global_step=1004, episodic_return=[-1.]
global_step=1020, episodic_return=[0.98776]
SPS:  113
global_step=1032, episodic_return=[-1.]
global_step=1040, episodic_return=[-1.]
global_step=1232, episodic_return=[-1.]
global_step=1276, episodic_return=[-1.]
global_step=1280, episodic_return=[-1.]
global_step=1308, episodic_return=[-1.]
global_step=1336, episodic_return=[-1.]
global_step=1344, episodic_return=[-1.]
global_step=1388, episodic_return=[-1.]
global_step=1412, episodic_return=[-1.]
global_step=1456, episodic_return=[-1.]
global_step=1492, episodic_return=[-1.]
global_step=1496, episodic_return=[-1.]
global_step=1512, episodic_return=[-1.]
global_step=1532, episodic_return=[-1.]
SPS:  128
global_step=1540, episodic_return=[-1.]
global_step=1584, episodic_return=[-1.]
global_step=1612, episodic_return=[-1.]
global_step=1688, episodic_return=[-1.]
global_step=1756, episodic_return=[-1.]
global_step=1804, episodic_return=[-1.]
global_step=1816, episodic_return=[-1.]
global_step=1820, episodic_return=[-1.]
global_step=1888, episodic_return=[-1.]
global_step=1924, episodic_return=[-1.]
global_step=1948, episodic_return=[-1.]
SPS:  139
global_step=2204, episodic_return=[-1.]
global_step=2228, episodic_return=[-1.]
global_step=2292, episodic_return=[-1.]
SPS:  141
global_step=2744, episodic_return=[-1.]
SPS:  134
global_step=3492, episodic_return=[-1.]
global_step=3508, episodic_return=[-1.]
global_step=3532, episodic_return=[-1.]
global_step=3580, episodic_return=[-1.]
SPS:  133
global_step=3752, episodic_return=[-1.]
global_step=3796, episodic_return=[-1.]
SPS:  126
global_step=4552, episodic_return=[-1.]
SPS:  124
SPS:  126
C:\Users\hxri\miniconda3\envs\cogppo\Lib\site-packages\numpy\core\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
C:\Users\hxri\miniconda3\envs\cogppo\Lib\site-packages\numpy\core\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
SPS:  126
global_step=6136, episodic_return=[-1.]
SPS:  130
SPS:  133
global_step=6872, episodic_return=[-1.]
global_step=7016, episodic_return=[-1.]
SPS:  136
global_step=7468, episodic_return=[-1.]
global_step=7536, episodic_return=[-1.]
global_step=7588, episodic_return=[-1.]
global_step=7636, episodic_return=[-1.]
SPS:  138
global_step=8016, episodic_return=[-1.]
global_step=8132, episodic_return=[-1.]
SPS:  140
SPS:  142
global_step=8928, episodic_return=[-1.]
SPS:  143
SPS:  145
global_step=9908, episodic_return=[-1.]
global_step=10004, episodic_return=[-1.]
SPS:  147
global_step=10472, episodic_return=[-1.]
SPS:  147
SPS:  149
SPS:  150
global_step=11940, episodic_return=[-1.]
global_step=12164, episodic_return=[-1.]
global_step=12184, episodic_return=[-1.]
global_step=12272, episodic_return=[-1.]
SPS:  152
SPS:  152
SPS:  152
SPS:  153
SPS:  154
global_step=14416, episodic_return=[-1.]
global_step=14548, episodic_return=[-1.]
SPS:  154
SPS:  154
SPS:  132
SPS:  133
SPS:  134
global_step=16932, episodic_return=[-1.]
SPS:  134
SPS:  133
SPS:  134
SPS:  134
SPS:  133
SPS:  134
SPS:  134
SPS:  134
SPS:  135
global_step=21644, episodic_return=[-1.]
SPS:  136
SPS:  135
SPS:  135
SPS:  134
SPS:  134
SPS:  135
SPS:  135
SPS:  135
SPS:  134
SPS:  134
SPS:  135
SPS:  135
SPS:  135
SPS:  135
SPS:  135
SPS:  135
SPS:  136
SPS:  135
SPS:  135
SPS:  135
SPS:  136
SPS:  136
SPS:  136
SPS:  136
SPS:  136
SPS:  136
SPS:  136
SPS:  136
SPS:  136
SPS:  136
SPS:  136
SPS:  135
SPS:  135
SPS:  134
SPS:  133
SPS:  132
SPS:  132
SPS:  132
global_step=41440, episodic_return=[-1.]
SPS:  132
SPS:  132
SPS:  132
Traceback (most recent call last):
  File "D:\Masters\Projects\thesis\cognitive-PPO\playground_gridworld.py", line 304, in <module>
    rewards[step] = torch.tensor(reward).to(device).view(-1)
                                                    ^^^^^^^^^
  File "C:\Users\hxri\miniconda3\envs\cogppo\Lib\site-packages\gymnasium\vector\vector_env.py", line 203, in step
    return self.step_wait()
           ^^^^^^^^^^^^^^^^
  File "C:\Users\hxri\miniconda3\envs\cogppo\Lib\site-packages\gymnasium\vector\sync_vector_env.py", line 149, in step_wait
    ) = env.step(action)
        ^^^^^^^^^^^^^^^^
  File "C:\Users\hxri\miniconda3\envs\cogppo\Lib\site-packages\gymnasium\wrappers\record_episode_statistics.py", line 89, in step
    ) = self.env.step(action)
        ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\hxri\miniconda3\envs\cogppo\Lib\site-packages\gymnasium\wrappers\order_enforcing.py", line 56, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\hxri\miniconda3\envs\cogppo\Lib\site-packages\gymnasium\wrappers\env_checker.py", line 49, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\Masters\Projects\thesis\cognitive-PPO\minigrid\minigrid\envs\dynamicobstacles.py", line 129, in step
    front_cell = self.grid.get(*self.front_pos)
                                ^^^^^^^^^^^^^^
  File "D:\Masters\Projects\thesis\cognitive-PPO\minigrid\minigrid\minigrid_env.py", line 1208, in front_pos
    return self.agent_pos + self.dir_vec
                            ^^^^^^^^^^^^
  File "D:\Masters\Projects\thesis\cognitive-PPO\minigrid\minigrid\minigrid_env.py", line 1183, in dir_vec
    @property
KeyboardInterrupt
SPS:  132